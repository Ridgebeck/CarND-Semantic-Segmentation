{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description of the project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project the task was to label individual pixels of images and output an entire image instead of just a classification. This was conducted with the help of a fully connected neural network or FCN for semantic segmentation. Semantic segmentation indentifies free space on the road at pixel-level granularity. The ultimate goal of this more detailed approach is to alow better decision making processes for driverless cars. In this project only two labels existed in which the pixels were categorized - road and no road. With more training data for various labels such as pedestrians, sidewalks, traffic lights, etc. this approach could be used to analyze pictures in great detail and help the diverless car to better understand complex environments. The following picture illustrates how this could look like."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Example for more labels](./images/semseg_example.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the high complexity and need for a lot of computational power, the project only focuses on the pixels being part of the road or not. Following a picture from the *kitti road* training set with the corresponding ground truth image and an overlay to see how it comes together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![training_sample](./images/training_sample.png)\n",
    "![training_sample_gt](./images/training_sample_gt.png)\n",
    "![training_sample_overlay](./images/training_sample_overlay.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pictures above show three labels in the ground truth image:\n",
    "* red - no road\n",
    "* pink - road the vehicle is driving on (both directions)\n",
    "* black - other roads\n",
    "But we are only focusing on two labels for now (road and no road). Therefore, the data was manipulated in the function *get_batches_fn* in *helper.py* before it was read into a numpy array. The red background was labeled as \"no road\" and everything else (the inverted part, that contains the pink and black areas) was labeled as \"road\" pixels.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to achieve results faster a pretrained and frozen VGG16 model was used as the base for the FCN. The goal of this project was to encode a picture, learn from it on a pixle level, and then decode the information into a new picture again. This was achieved with the help of a pretrained model and a 1x1 convolution as the encoder and transposed convolutions as the decoder to upsample the image data back to the original format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![General structure of the FCN](./images/FCN_structure.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this project was to take the frozen VGG16 and add skip connections, 1x1 convolutions, and transposed convolutions. I therefore added skip connections to the VGG layers 3, 4, and 7 and added a 1x1 convolutional layer to each skipped connection. After that I added upsampling (transposed convolutions) to the output of the highest layer (convolution on layer 7) and added it (element-wise) to the output of the convolution on the next highest layer (layer 4). Then I took the result and did the same with the output from layer 3. A complete structure of the FCN can be seen in the picture below, which is a graph I visualized with the help of TensorBoard. The green box shows the added layers to the pretrained VGG16."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Graph of overall network structure.](./images/complete_network_graph.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following section shows the program code of *main.py* divided up into different sections with additional explanation. The results are shown and explained below the program code to show the improvements that were made while developing the code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import all necessary dependencies and checking tensorflow and GPU support"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I imported all necessary dependencies and made sure that the TensorFlow version was coorect and the GPU was found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 1.6.0\n",
      "Default GPU Device: /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "import os.path\n",
    "import tensorflow as tf\n",
    "import helper\n",
    "import warnings\n",
    "from distutils.version import LooseVersion\n",
    "import project_tests as tests\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Check TensorFlow Version\n",
    "assert LooseVersion(tf.__version__) >= LooseVersion('1.0'), 'Please use TensorFlow version 1.0 or newer.  You are using {}'.format(tf.__version__)\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))\n",
    "\n",
    "# Check for a GPU\n",
    "if not tf.test.gpu_device_name():\n",
    "    warnings.warn('No GPU found. Please use a GPU to train your neural network.')\n",
    "else:\n",
    "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load pretrained VGG model into TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, the function for loading the pretrained VGG16 model was defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vgg(sess, vgg_path):\n",
    "    \"\"\"\n",
    "    Load Pretrained VGG Model into TensorFlow.\n",
    "    :param sess: TensorFlow Session\n",
    "    :param vgg_path: Path to vgg folder, containing \"variables/\" and \"saved_model.pb\"\n",
    "    :return: Tuple of Tensors from VGG model (image_input, keep_prob, layer3_out, layer4_out, layer7_out)\n",
    "    \"\"\"\n",
    "    # TODO: Implement function\n",
    "    #   Use tf.saved_model.loader.load to load the model and weights\n",
    "    vgg_tag = 'vgg16'\n",
    "    vgg_input_tensor_name = 'image_input:0'\n",
    "    vgg_keep_prob_tensor_name = 'keep_prob:0'\n",
    "    vgg_layer3_out_tensor_name = 'layer3_out:0'\n",
    "    vgg_layer4_out_tensor_name = 'layer4_out:0'\n",
    "    vgg_layer7_out_tensor_name = 'layer7_out:0'\n",
    "\n",
    "    tf.saved_model.loader.load(sess, [vgg_tag], vgg_path)\n",
    "    graph = tf.get_default_graph()\n",
    "    w1 = graph.get_tensor_by_name(vgg_input_tensor_name)\n",
    "    keep = graph.get_tensor_by_name(vgg_keep_prob_tensor_name)\n",
    "    layer_3 = graph.get_tensor_by_name(vgg_layer3_out_tensor_name)\n",
    "    layer_4 = graph.get_tensor_by_name(vgg_layer4_out_tensor_name)\n",
    "    layer_7 = graph.get_tensor_by_name(vgg_layer7_out_tensor_name)\n",
    "    \n",
    "    return w1, keep, layer_3, layer_4, layer_7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function was seperately tested with the already implemented test function in *project_tests.py*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "tests.test_load_vgg(load_vgg, tf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the layers for a fully convolutional network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following I had to implement the function *layers()* where the 1x1 convolution and the upsampling of the different layers was implemented as described before. I used a kernel regularizer on every layer as recommended in the course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layers(vgg_layer3_out, vgg_layer4_out, vgg_layer7_out, num_classes):\n",
    "    \"\"\"\n",
    "    Create the layers for a fully convolutional network.  Build skip-layers using the vgg layers.\n",
    "    :param vgg_layer3_out: TF Tensor for VGG Layer 3 output\n",
    "    :param vgg_layer4_out: TF Tensor for VGG Layer 4 output\n",
    "    :param vgg_layer7_out: TF Tensor for VGG Layer 7 output\n",
    "    :param num_classes: Number of classes to classify\n",
    "    :return: The Tensor for the last layer of output\n",
    "    \"\"\"\n",
    "    # TODO: Implement function\n",
    "    # 1x1 convolution of vgg layer 7\n",
    "    layer7a_out = tf.layers.conv2d(vgg_layer7_out, num_classes, 1, \n",
    "                                   padding= 'same', \n",
    "                                   kernel_initializer= tf.random_normal_initializer(stddev=0.01),\n",
    "                                   kernel_regularizer= tf.contrib.layers.l2_regularizer(1e-3))\n",
    "    # upsample\n",
    "    layer4a_in1 = tf.layers.conv2d_transpose(layer7a_out, num_classes, 4, \n",
    "                                             strides= (2, 2), \n",
    "                                             padding= 'same', \n",
    "                                             kernel_initializer= tf.random_normal_initializer(stddev=0.01), \n",
    "                                             kernel_regularizer= tf.contrib.layers.l2_regularizer(1e-3))\n",
    "    # make sure the shapes are the same!\n",
    "    # 1x1 convolution of vgg layer 4\n",
    "    layer4a_in2 = tf.layers.conv2d(vgg_layer4_out, num_classes, 1, \n",
    "                                   padding= 'same', \n",
    "                                   kernel_initializer= tf.random_normal_initializer(stddev=0.01), \n",
    "                                   kernel_regularizer= tf.contrib.layers.l2_regularizer(1e-3))\n",
    "    # element-wise addition\n",
    "    layer4a_out = tf.add(layer4a_in1, layer4a_in2)\n",
    "    # upsample\n",
    "    layer3a_in1 = tf.layers.conv2d_transpose(layer4a_out, num_classes, 4,  \n",
    "                                             strides= (2, 2), \n",
    "                                             padding= 'same', \n",
    "                                             kernel_initializer= tf.random_normal_initializer(stddev=0.01), \n",
    "                                             kernel_regularizer= tf.contrib.layers.l2_regularizer(1e-3))\n",
    "    # 1x1 convolution of vgg layer 3\n",
    "    layer3a_in2 = tf.layers.conv2d(vgg_layer3_out, num_classes, 1, \n",
    "                                   padding= 'same', \n",
    "                                   kernel_initializer= tf.random_normal_initializer(stddev=0.01), \n",
    "                                   kernel_regularizer= tf.contrib.layers.l2_regularizer(1e-3))\n",
    "    # element-wise addition\n",
    "    layer3a_out = tf.add(layer3a_in1, layer3a_in2)\n",
    "    # upsample\n",
    "    nn_last_layer = tf.layers.conv2d_transpose(layer3a_out, num_classes, 16,  \n",
    "                                               strides= (8, 8), \n",
    "                                               padding= 'same', \n",
    "                                               kernel_initializer= tf.random_normal_initializer(stddev=0.01), \n",
    "                                               kernel_regularizer= tf.contrib.layers.l2_regularizer(1e-3))\n",
    "    return nn_last_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function was seperately tested with the already implemented test function in project_tests.py."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "tests.test_layers(layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the TensorFlow loss and optimizer operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then I had to define the function *optimize()*, where the result from *layers()* was taken in order to calculate the loss, the accuracy, and the tyraining operation. I decided to take the cross entropy loss and use an Adam optimizer as this was recommended in the lessons. I also added the loss and accuracy to a summary that I used later for visualizing the performance of the FCN in TensorBoard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize(nn_last_layer, correct_label, learning_rate, num_classes):\n",
    "    \"\"\"\n",
    "    Build the TensorFlow loss and optimizer operations.\n",
    "    :param nn_last_layer: TF Tensor of the last layer in the neural network\n",
    "    :param correct_label: TF Placeholder for the correct label image\n",
    "    :param learning_rate: TF Placeholder for the learning rate\n",
    "    :param num_classes: Number of classes to classify\n",
    "    :return: Tuple of (logits, train_op, cross_entropy_loss)\n",
    "    \"\"\"\n",
    "    # TODO: Implement function\n",
    "    # make logits a 2D tensor where each row represents a pixel and each column a class\n",
    "    logits = tf.reshape(nn_last_layer, (-1, num_classes))\n",
    "    correct_label = tf.reshape(correct_label, (-1,num_classes))\n",
    "\n",
    "    # define loss function\n",
    "    with tf.name_scope('cross_entropy'):\n",
    "        cross_entropy_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits= logits, labels= correct_label))\n",
    "\n",
    "    # calculate accuracy\n",
    "    with tf.name_scope('accuracy'):\n",
    "        correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(correct_label, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "    # define training operation\n",
    "    with tf.name_scope('train'):\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate= learning_rate)\n",
    "        train_op = optimizer.minimize(cross_entropy_loss)\n",
    "\n",
    "    tf.summary.scalar(\"cost\", cross_entropy_loss)\n",
    "    tf.summary.scalar(\"accuracy\", accuracy)\n",
    "    \n",
    "\n",
    "    return logits, train_op, cross_entropy_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function was seperately tested with the already implemented test function in project_tests.py."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-6-724f53aae520>:17: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See tf.nn.softmax_cross_entropy_with_logits_v2.\n",
      "\n",
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "tests.test_optimize(optimize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add preprocessing steps here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train neural network and print out the loss during training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After that I implemented the training process where the FCN was trained on the Kitti road dataset in batches. All summaries were merged and written to a file for later evaluation with TensorBoard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_nn(sess, epochs, batch_size, get_batches_fn, train_op, cross_entropy_loss, input_image,\n",
    "             correct_label, keep_prob, learning_rate):\n",
    "    \"\"\"\n",
    "    Train neural network and print out the loss during training.\n",
    "    :param sess: TF Session\n",
    "    :param epochs: Number of epochs\n",
    "    :param batch_size: Batch size\n",
    "    :param get_batches_fn: Function to get batches of training data.  Call using get_batches_fn(batch_size)\n",
    "    :param train_op: TF Operation to train the neural network\n",
    "    :param cross_entropy_loss: TF Tensor for the amount of loss\n",
    "    :param input_image: TF Placeholder for input images\n",
    "    :param correct_label: TF Placeholder for label images\n",
    "    :param keep_prob: TF Placeholder for dropout keep probability\n",
    "    :param learning_rate: TF Placeholder for learning rate\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO: Implement function\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    summary_op = tf.summary.merge_all()\n",
    "    writer = tf.summary.FileWriter(\"./writer/TensorBoard\", graph=tf.get_default_graph())\n",
    "    \n",
    "    print(\"Training...\")\n",
    "\n",
    "    step = 0\n",
    "    for i in range(epochs):\n",
    "        print(\"EPOCH {} ...\".format(i+1))\n",
    "\n",
    "        for image, label in get_batches_fn(batch_size):         \n",
    "\n",
    "            _, loss = sess.run([train_op, summary_op], feed_dict={input_image: image, correct_label: label, keep_prob: 0.5, learning_rate: 0.0009})\n",
    "\n",
    "            writer.add_summary(loss, step)\n",
    "            \n",
    "            #tf.summary.image(\"input\", image, 1)\n",
    "            #tf.summary.image(\"label\", label, 1)\n",
    "            \n",
    "            #print(\"Batch: \" + str(step+1))\n",
    "            step = step + 1\n",
    "            \n",
    "\n",
    "            #print(\"Loss: = {:.3f}\".format(loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test function was commented out due to the fact that the modifications for TensorBoard would cause an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tests.test_train_nn(train_nn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the training data was very limited (only 289 pictures) I decided to augment the pictures randomly to increase the robustness of the algorithm. I added the function *modify_picture()* to *helper.py* for that purpose, that was called in the function *get_batches_fn()* when training the neural network in batches  and randomly flipped and rotated the pictures and the corresponding ground truth image before passing the data to the FCN. Following an example picture that was flipped horizontally and then rotated. I limited the rotation to +/- 5 degrees to keep it in a more realistic range."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Original picture:\n",
    "![Normal picture](./images/training_picture_normal.png)\n",
    "\n",
    "Horizontally flipped:\n",
    "![Flipped picture](./images/training_picture_flipped.png)\n",
    "\n",
    "And rotated:\n",
    "![Normal picture](./images/training_picture_rotated.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally the function *run()* was implemented to put everything together - from loading the pretrained VGG16 and the dataset to training the modified FCN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n",
      "Pretrained vgg model found.\n",
      "INFO:tensorflow:Restoring parameters from b'./data\\\\vgg\\\\variables\\\\variables'\n",
      "INFO:tensorflow:Restoring parameters from b'./data\\\\vgg\\\\variables\\\\variables'\n",
      "Training...\n",
      "EPOCH 1 ...\n",
      "EPOCH 2 ...\n",
      "EPOCH 3 ...\n",
      "Training Finished. Saving test images to: ./runs\\1523841297.566543\n"
     ]
    }
   ],
   "source": [
    "def run():\n",
    "\n",
    "    # reset graph\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    num_classes = 2 # road and no-road\n",
    "    image_shape = (160, 576)\n",
    "    data_dir = './data'\n",
    "    runs_dir = './runs'\n",
    "    tests.test_for_kitti_dataset(data_dir)\n",
    "\n",
    "    # Download pretrained vgg model\n",
    "    helper.maybe_download_pretrained_vgg(data_dir)\n",
    "\n",
    "    # OPTIONAL: Train and Inference on the cityscapes dataset instead of the Kitti dataset.\n",
    "    # You'll need a GPU with at least 10 teraFLOPS to train on.\n",
    "    #  https://www.cityscapes-dataset.com/\n",
    "\n",
    "    epochs = 25\n",
    "    batch_size = 10\n",
    "\n",
    "    training_image_path = os.path.join(data_dir, 'data_road/training/image_2')\n",
    "    training_image_no = len(os.listdir(training_image_path))\n",
    "\n",
    "    # tf placeholders\n",
    "    learning_rate = tf.placeholder(tf.float32, name='learning_rate')\n",
    "    correct_label = tf.placeholder(tf.int32, [None, None, None, num_classes], name='correct_label')\n",
    "\n",
    "\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        # Path to vgg model\n",
    "        vgg_path = os.path.join(data_dir, 'vgg')\n",
    "        # Create function to get batches\n",
    "        get_batches_fn = helper.gen_batch_function(os.path.join(data_dir, 'data_road/training'), image_shape)\n",
    "\n",
    "        # load vgg\n",
    "        vgg_tag = 'vgg16'\n",
    "        tf.saved_model.loader.load(sess, [vgg_tag], vgg_path)\n",
    "\n",
    "        # OPTIONAL: Augment Images for better results\n",
    "        #  https://datascience.stackexchange.com/questions/5224/how-to-prepare-augment-images-for-neural-network\n",
    "\n",
    "        # TODO: Build NN using load_vgg, layers, and optimize function\n",
    "\n",
    "        input_image, keep_prob, vgg_layer3_out, vgg_layer4_out, vgg_layer7_out = load_vgg(sess, vgg_path)\n",
    "\n",
    "        nn_last_layer = layers(vgg_layer3_out, vgg_layer4_out, vgg_layer7_out, num_classes)\n",
    "\n",
    "        logits, train_op, cross_entropy_loss = optimize(nn_last_layer, correct_label, learning_rate, num_classes)\n",
    "\n",
    "        # TODO: Train NN using the train_nn function\n",
    "        train_nn(sess, epochs, batch_size, get_batches_fn, train_op, cross_entropy_loss, input_image, correct_label, keep_prob, learning_rate)\n",
    "\n",
    "        # TODO: Save inference data using helper.save_inference_samples\n",
    "        helper.save_inference_samples(runs_dir, data_dir, sess, image_shape, logits, keep_prob, input_image)\n",
    "\n",
    "        # OPTIONAL: Apply the trained model to a video\n",
    "\n",
    "        print('All finished.')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I trained the FCN in steps to see the progress of the learning along the way. I started with only 1 Epoch, then increased it steadily to up to 25 Epochs. The pictures below show the overlayed prediction of the road pixels on the original image after the different epochs. A lot of change happened during the first few epochs and therefore I chose to show epochs 1,2,3,5 and 25."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "After 1 Epoch:\n",
    "![Picture 1 Epoch](./images/1_epoch.png)\n",
    "\n",
    "After 2 Epochs:\n",
    "![Picture 2 Epochs](./images/2_epochs.png)\n",
    "\n",
    "After 3 Epochs:\n",
    "![Picture 3 Epochs](./images/3_epochs.png)\n",
    "\n",
    "After 5 Epochs:\n",
    "![Picture 5 Epochs](./images/5_epochs.png)\n",
    "\n",
    "After 25 Epochs:\n",
    "![Picture 25 Epochs](./images/25_epochs.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be seen that after the first epoch the prediction was almost a random guess. The red pixels seem to be almost evenly distributed. After the second epoch, most of the pixels were gone and only a few appeared in the center where the algorithm was certain that they belonged to the road. After the third epoch, a lot more pixels show up, but they are not going all the way to sides of the road and also not into the distance. You can clearly see larger steps forming the diagonal limits. It also recognized the pull-off area on the right falsy as a part of the road. After 5 epochs the steps were smoother and it detected into further distance. After 25 epochs it was way smoother, the pull-off area was removed and some parts of the road on the other side were detected.\n",
    "\n",
    "To show the progress over the 25 epochs, I recorded the accuracy and cost for each batch that was used for training. I trained for 25 Epochs with a batch size of 10. For 289 images that meant 29 batches per epoch or 725 batches in total. The images of the final test with the overlayed detected road pixels were saved as well as the summary of the accuracy and cost for TensorBoard. Following the two graphs:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Accuracy:\n",
    "![TensorBoard Accuracy](./images/tensorboard_accuracy.png)\n",
    "\n",
    "Cost:\n",
    "![TensorBoard Cost](./images/tensorboard_cost.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following a couple of examples where the algorithm did a good job detecting the road pixels:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "It detected multiple lanes in both directions and was not distracted by the shadows of the trees:\n",
    "![Good example 1](./images/good_example_1.png)\n",
    "\n",
    "It omitted the pixels of the cars that were on the street with good accuracy (there are still some red pixels in the car):\n",
    "![Good example 2](./images/good_example_2.png)\n",
    "\n",
    "It recognized that the sidewalk to the right does not belong to the road even though it has a very similar color and shape:\n",
    "![Good example 3](./images/good_example_3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here are some examples were the algorithm failed to detect a significant amount of the correct road pixels:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "It recognized the sidewalk as part of the road and did not recognized the parts of the road in the shadows:\n",
    "![Bad example 1](./images/bad_example_1.png)\n",
    "\n",
    "Again, the shadows were throwing off the road recognition:\n",
    "![Bad example 2](./images/bad_example_2.png)\n",
    "\n",
    "The different light conditions of the underpass caused some problems, too:\n",
    "![Bad example 3](./images/bad_example_3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The biggest problem seemed to be hard light and shadows on the street caused by buildings, trees, cars etc. This is understandable as only a few pictures in the training data showed shadows and the algorithm was not able to learn properly from that. In order to achieve better performance the solution could be further tweaked in multiple ways. Additional steps could be added to the image preprocessing such as random brightness adjustments of the whole picture or of parts of the picture (to create artificial shadows). The pictures could also be normalized and transferred into a different color space. But even more important is to train on a larger data set, as only 289 pictures is not enough to cover all the different traffic situations."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
